{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "tree = pd.read_csv('tree.csv')\n",
    "tree['t'] = tree['t'].replace(to_replace=0, value=0.1)\n",
    "\n",
    "vert_genes = pd.read_csv('vert_genes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the graph\n",
    "\n",
    "def create_graph(tree, alpha, beta, sigma_sq):\n",
    "    G = nx.DiGraph()\n",
    "    for _, row in tree.iterrows():\n",
    "        if not pd.isna(row['Parent']):\n",
    "            G.add_edge(int(row[\"Parent\"]), int(row[\"Child\"]), time = row[\"t\"], a = alpha*row[\"t\"], b = beta, variance = sigma_sq*row[\"t\"])\n",
    "            \n",
    "    return G\n",
    "\n",
    "G = create_graph(tree, alpha = 0, beta = 1, sigma_sq = 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_node_length_with_parameters(G, parent, simulated_lengths, alpha, beta, sigma_sq):\n",
    "    for child in G.successors(parent):\n",
    "        t = G[parent][child]['time']\n",
    "        mean = alpha * t + beta * simulated_lengths[parent]\n",
    "        std = np.sqrt(sigma_sq * t)\n",
    "        simulated_lengths[child] = np.random.normal(mean, std)\n",
    "        simulate_node_length_with_parameters(G, child, simulated_lengths, alpha, beta, sigma_sq)\n",
    "    \n",
    "    return simulated_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 204)\n",
      "(1000,)\n",
      "49930.60234858928\n"
     ]
    }
   ],
   "source": [
    "def simulate_data_for_learning(G, n, alpha, beta, sigma_sq, alpha_0, sigma_0_sq, root, learn_params = True, only_X = True):\n",
    "    X_values = []\n",
    "    Y_values = []\n",
    "    \n",
    "    all_nodes = list(G.nodes)  # Get all nodes in the graph\n",
    "\n",
    "    for _ in range(n):\n",
    "        simulated_lengths = {}\n",
    "        \n",
    "        # Simulate root node first\n",
    "        simulated_lengths[root] = np.random.normal(alpha_0, np.sqrt(sigma_0_sq))\n",
    "        \n",
    "        # Simulate all other nodes recursively\n",
    "        simulated_lengths = simulate_node_length_with_parameters(G, root, simulated_lengths, alpha, beta, sigma_sq)\n",
    "        if only_X:\n",
    "            leaf_nodes = [node for node in all_nodes if G.out_degree(node) == 0]\n",
    "            simulated_x = [simulated_lengths[node] for node in leaf_nodes]\n",
    "            \n",
    "            X_values.append(simulated_x)\n",
    "        else:\n",
    "            X_values.append([simulated_lengths[node] for node in all_nodes])\n",
    "            \n",
    "        if learn_params:\n",
    "            Y_values.append([alpha,beta,sigma_sq])\n",
    "        else:\n",
    "            Y_values.append(simulated_lengths[root])\n",
    "\n",
    "    return np.array(X_values), np.array(Y_values)\n",
    "\n",
    "learn_parameters = False\n",
    "\n",
    "X, y = simulate_data_for_learning(G, 1000, alpha = 0.5, beta = 1, sigma_sq = 2500, alpha_0 = 50000, sigma_0_sq = 5000, root = 407, learn_params=learn_parameters)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(y[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gamma(G):\n",
    "    gamma = np.eye(len(G.nodes)) # Initialize gamma as an identity matrix\n",
    "\n",
    "    # Iterate through the nodes in the graph\n",
    "    for node in G.nodes:\n",
    "        parent = next(G.predecessors(node), None)\n",
    "        if parent is None:\n",
    "            continue\n",
    "        else:\n",
    "            gamma[parent-1, node -1] = -G[parent][node]['b'] # Determine the dependency between parent and child nodes with -b\n",
    "    return gamma\n",
    "\n",
    "def compute_beta(G, alpha_0):\n",
    "    beta = np.zeros((len(G.nodes), 1))\n",
    "    for node in G.nodes:\n",
    "        parent = next(G.predecessors(node), None)\n",
    "        if parent is None:\n",
    "            beta[node-1] = alpha_0\n",
    "            continue\n",
    "        a = G[parent][node]['a'] # The constant term in the mean of the CPD\n",
    "        beta[node-1] = a\n",
    "        \n",
    "    return beta\n",
    "\n",
    "def compute_sigma(G, sigma_0_sq):\n",
    "    sigma = np.zeros((len(G.nodes)))\n",
    "    for node in G.nodes:\n",
    "        parent = next(G.predecessors(node), None)\n",
    "        if parent is None:\n",
    "            # Assign the default value for the root node\n",
    "            sigma[node - 1] = sigma_0_sq\n",
    "        else:\n",
    "            # Access the edge attribute 'variance' only if parent exists\n",
    "            variance = G[parent][node]['variance']\n",
    "            sigma[node - 1] = variance\n",
    "    return sigma\n",
    "\n",
    "def compute_J_and_h(alpha, beta, sigma_sq, sigma_0_sq, alpha_0):\n",
    "    \n",
    "    G = create_graph(tree, alpha, beta, sigma_sq)\n",
    "\n",
    "    beta = compute_beta(G, alpha_0)\n",
    "    sigma = compute_sigma(G, sigma_0_sq)\n",
    "    gamma = compute_gamma(G)\n",
    "\n",
    "    J = np.sum([np.outer(gamma[:, i], gamma[:, i]) / sigma[i] for i in range(len(G))], axis=0)\n",
    "    h = np.sum([(beta[i] / sigma[i]) * gamma[:, i] for i in range(len(G))], axis=0)\n",
    "    return J, h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(407, 407) (407, 407)\n",
      "Difference between empirical and computed covariance matrix \n",
      ": [[ -23598.74283951   -2077.61413687    7239.68642705 ...    3916.47944992\n",
      "     4297.87589844    5147.61419699]\n",
      " [  -2077.61413687   -3402.99201237    8230.37400879 ...    1906.74810246\n",
      "     3401.22724416    4129.24387502]\n",
      " [   7239.68642705    8230.37400879    1275.1482611  ...    6108.09979759\n",
      "     3168.41440028    3921.71192039]\n",
      " ...\n",
      " [   3916.47944992    1906.74810246    6108.09979759 ...   -7325.80000603\n",
      "  -104014.36528349    2440.75939477]\n",
      " [   4297.87589844    3401.22724416    3168.41440028 ... -104014.36528349\n",
      "   -20057.65056599   93384.38969634]\n",
      " [   5147.61419699    4129.24387502    3921.71192039 ...    2440.75939477\n",
      "    93384.38969634   98439.23639781]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0., 10.])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1000\n",
    "\n",
    "alpha = 0\n",
    "beta = 1\n",
    "sigma_sq = 2500\n",
    "All_nodes_simulated, _ = simulate_data_for_learning(G, n, alpha=alpha, beta=beta, sigma_sq=sigma_sq, alpha_0=50000, sigma_0_sq=5000, root=407, learn_params=False, only_X=False)\n",
    "empirical_covariance_matrix = np.cov(All_nodes_simulated, rowvar=False)\n",
    "\n",
    "J, h = compute_J_and_h(alpha = alpha, beta = beta, sigma_sq = sigma_sq, sigma_0_sq=5000, alpha_0=50000)\n",
    "\n",
    "computed_covariance_matrix = np.linalg.inv(J)\n",
    "\n",
    "print(empirical_covariance_matrix.shape, computed_covariance_matrix.shape)\n",
    "\n",
    "\n",
    "difference =  empirical_covariance_matrix - computed_covariance_matrix\n",
    "print(f\"Difference between empirical and computed covariance matrix \\n: {difference}\")\n",
    "\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clique_tree(G):\n",
    "    C = nx.Graph()\n",
    "\n",
    "    G_working = G.copy()\n",
    "    leaves = [node for node in G_working.nodes if G_working.out_degree(node) == 0]\n",
    "    G_working.remove_nodes_from(leaves)\n",
    "\n",
    "    index = min(G_working.nodes) -1 if all(isinstance(n, int) for n in G_working.nodes) else 0\n",
    "\n",
    "    for node in G_working.nodes:\n",
    "        parent = node\n",
    "        children = list(G_working.neighbors(parent))\n",
    "        C.add_node(parent, variables=[parent])\n",
    "        for child in children:\n",
    "            pair_clique = index\n",
    "            C.add_node(pair_clique, variables=[parent, child])\n",
    "            C.add_edge(parent, pair_clique)\n",
    "            C.add_edge(pair_clique, child)\n",
    "            index = index - 1\n",
    "\n",
    "    return C\n",
    "\n",
    "C = compute_clique_tree(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Away from zero:  205\n",
      "Max index:  407\n",
      "Min index:  205\n",
      "Number of variables in the graph:  202\n"
     ]
    }
   ],
   "source": [
    "NoV = len([node for node in C.nodes if len(C.nodes[node]['variables']) == 1]) -1\n",
    "maxIndex = max([node for node in C.nodes])\n",
    "away_from_zero = maxIndex - NoV\n",
    "minIndex = min([node for node in C.nodes if len(C.nodes[node]['variables']) == 1])\n",
    "\n",
    "print(\"Away from zero: \", away_from_zero)\n",
    "print(\"Max index: \", maxIndex)\n",
    "print(\"Min index: \", minIndex)\n",
    "print(\"Number of variables in the graph: \", NoV)\n",
    "\n",
    "def mapping_GTM(index_in_graph):\n",
    "    return index_in_graph - away_from_zero \n",
    "\n",
    "def mapping_MTG(index_in_matrix):\n",
    "    return index_in_matrix + away_from_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix algebra implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_matrices(scope, X_values, J, h):\n",
    "    X_indices = np.isin(scope, X_values)\n",
    "    Z_indices = ~X_indices\n",
    "\n",
    "    J_ZZ = J[Z_indices, :][:, Z_indices]\n",
    "    J_ZX = J[Z_indices, :][:, X_indices]\n",
    "    J_XZ = J_ZX.T\n",
    "    J_XX = J[X_indices, :][:, X_indices]\n",
    "    J_ZZ_inv = np.linalg.inv(J_ZZ)\n",
    "\n",
    "    h_X = h[X_indices]\n",
    "    h_Z = h[Z_indices]\n",
    "\n",
    "    return J_ZZ, J_ZX, J_XZ, J_XX, J_ZZ_inv, h_X, h_Z\n",
    "\n",
    "def get_conditional_distribution(J, h, X_values, X_indices):\n",
    "    scope = [i for i in range(1, len(J) + 1)]\n",
    "    J_ZZ, J_ZX, J_XZ, J_XX, J_ZZ_inv, h_X, h_Z = get_sub_matrices(scope, X_indices, J, h)\n",
    "\n",
    "    J_reduced = J_ZZ\n",
    "    h_reduced = h_Z- J_ZX @ X_values\n",
    "\n",
    "    return J_reduced, h_reduced\n",
    "\n",
    "X_values = simulate_data_for_learning(G, 1, alpha=alpha, beta=beta, sigma_sq=sigma_sq, alpha_0=50000, sigma_0_sq=5000, root=407, learn_params=False, only_X=True)[0]\n",
    "\n",
    "\n",
    "leaves = [node for node in G.nodes if G.out_degree(node) == 0]\n",
    "X_indices = leaves\n",
    "\n",
    "J_reduced, h_reduced = get_conditional_distribution(J, h, X_values[0], X_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value for node 407: 49792.31898955036\n",
      "Variance for node 407: 5926.1487494450475\n",
      "Actual value for node 407: 49398.97361219418\n",
      "True variance for node 407: 2500\n"
     ]
    }
   ],
   "source": [
    "Sigma = np.linalg.inv(J_reduced)\n",
    "mu = Sigma @ h_reduced\n",
    "\n",
    "random_index = np.random.choice(range(len(X_values[0])))\n",
    "z = mapping_GTM(random_index)\n",
    "print(f\"Predicted value for node 407: {mu[z]}\")\n",
    "print(f\"Variance for node 407: {Sigma[z,z]}\")\n",
    "print(f\"Actual value for node 407: {X_values[0][0]}\")\n",
    "print(f\"True variance for node 407: {sigma_sq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II : inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True mean: 49560.2929693672\n",
      "True std dev: 2500\n",
      "Posterior mean: 49950.05573773816\n",
      "Posterior std dev: 60.085211992236054\n"
     ]
    }
   ],
   "source": [
    "def single_clique(G):\n",
    "    leaves = [node for node in G.nodes if G.out_degree(node) == 0]\n",
    "    H = G.copy()\n",
    "    H.remove_nodes_from(leaves)\n",
    "    H = H.to_undirected()\n",
    "    return H\n",
    "\n",
    "def compute_J_i_arrow_j(clique_tree, i, j, J, h, J_messages, h_messages):\n",
    "    \n",
    "    neighbors = list(clique_tree.neighbors(i))\n",
    "    neighbors.remove(j)\n",
    "\n",
    "    i_idx = mapping_GTM(i)\n",
    "    j_idx = mapping_GTM(j)\n",
    "\n",
    "    if not neighbors:\n",
    "        J_messages[i_idx][j_idx] = -J[i_idx, j_idx] * J[j_idx, i_idx] / J[i_idx, i_idx]\n",
    "    else:\n",
    "        J_sum = sum(compute_J_i_arrow_j(clique_tree, k, i, J, h, J_messages, h_messages) for k in neighbors)\n",
    "        J_messages[i_idx][j_idx] = -J[i_idx, j_idx] * J[j_idx, i_idx] / (J[i_idx, i_idx] + J_sum)\n",
    "\n",
    "    return J_messages[i_idx][j_idx]\n",
    "\n",
    "\n",
    "def compute_h_i_arrow_j(clique_tree, i, j, J, h, J_messages, h_messages):\n",
    "\n",
    "    neighbors = list(clique_tree.neighbors(i))\n",
    "    neighbors.remove(j)\n",
    "\n",
    "    i_idx = mapping_GTM(i)\n",
    "    j_idx = mapping_GTM(j)\n",
    "\n",
    "    if not neighbors:\n",
    "        h_messages[i_idx][j_idx] = -J[i_idx, j_idx] * h[i_idx] / J[i_idx, i_idx]\n",
    "    else:\n",
    "        J_sum = sum(compute_J_i_arrow_j(clique_tree, k, i, J, h, J_messages, h_messages) for k in neighbors)\n",
    "        h_sum = sum(compute_h_i_arrow_j(clique_tree, k, i, J, h, J_messages, h_messages) for k in neighbors)\n",
    "        Ji_backslash_j = J[i_idx, i_idx] + J_sum\n",
    "        hi_backslash_j = h[i_idx] + h_sum\n",
    "        h_messages[i_idx][j_idx] = (-J[i_idx, j_idx] * hi_backslash_j) / (Ji_backslash_j)\n",
    "\n",
    "    return h_messages[i_idx][j_idx]\n",
    "\n",
    "def inference_algorithm(G, alpha, beta, sigma_sq, alpha_0, observed_X, Z):\n",
    "\n",
    "    clique_tree = single_clique(G)\n",
    "    J, h = compute_J_and_h(alpha, beta, sigma_sq, 5000, alpha_0)\n",
    "\n",
    "    n_observed = len(observed_X)\n",
    "    J_reduced, h_reduced = get_conditional_distribution(J, h, observed_X, list(range(1, n_observed + 1)))\n",
    "\n",
    "    J_messages = np.full(J_reduced.shape, np.nan)\n",
    "    h_messages = np.full(J_reduced.shape, np.nan)\n",
    "\n",
    "    Z_neighbors = list(clique_tree.neighbors(Z))\n",
    "    Z_idx = mapping_GTM(Z)\n",
    "\n",
    "    J_zz = J_reduced[Z_idx, Z_idx]\n",
    "\n",
    "    sum_J = sum([compute_J_i_arrow_j(clique_tree, k, Z, J_reduced, h_reduced, J_messages, h_messages) for k in Z_neighbors])\n",
    "    sum_h = sum([compute_h_i_arrow_j(clique_tree, k, Z, J_reduced, h_reduced, J_messages, h_messages) for k in Z_neighbors])\n",
    "\n",
    "    J_hat_Z = J_zz + sum_J\n",
    "    h_hat_Z = h_reduced[Z_idx] + sum_h\n",
    "\n",
    "\n",
    "    return J_hat_Z, h_hat_Z\n",
    "\n",
    "def information_to_standard(J_hat_Z, h_hat_Z, Z):\n",
    "\n",
    "    mu = h_hat_Z / J_hat_Z\n",
    "    sigma = np.sqrt(1 / J_hat_Z)\n",
    "    return mu, sigma\n",
    "\n",
    "n = 10\n",
    "random_index = np.random.choice(range(n))\n",
    "Z = 407  # root node\n",
    "\n",
    "alpha = 0\n",
    "beta = 1\n",
    "sigma_sq = 2500\n",
    "alpha_0 = 50000\n",
    "\n",
    "X_values, _ = simulate_data_for_learning(G, n, alpha = alpha, beta = beta, sigma_sq = sigma_sq, alpha_0 = 50000, sigma_0_sq = 5000, root = 407, learn_params=False, only_X=True)\n",
    "\n",
    "\n",
    "J_hat_Z, h_hat_Z = inference_algorithm(G, alpha, beta, sigma_sq, alpha_0, X_values[random_index], Z)\n",
    "mu, sigma = information_to_standard(J_hat_Z, h_hat_Z, Z)\n",
    "\n",
    "true_mu = X_values[random_index][0]\n",
    "true_sigma = sigma_sq\n",
    "print(\"True mean:\", true_mu)\n",
    "print(\"True std dev:\", true_sigma)\n",
    "print(\"Posterior mean:\", mu)\n",
    "print(\"Posterior std dev:\", sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: 0.5601814019849685\n",
      "Estimated beta: 0.9999954093606939\n",
      "Estimated sigma_sq: 2502.2942123815674\n"
     ]
    }
   ],
   "source": [
    "def simulate_full_data(G, n_samples, alpha, beta, sigma_sq, alpha_0, sigma_0_sq):\n",
    "    rows = []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        simulated = {}\n",
    "        simulated[root] = np.random.normal(alpha_0, np.sqrt(sigma_0_sq))\n",
    "        simulate_node_length_with_parameters(G, root, simulated, alpha, beta, sigma_sq)\n",
    "\n",
    "        for parent, child in G.edges:\n",
    "            t = G[parent][child]['time']\n",
    "            y = simulated[child]\n",
    "            z = simulated[parent]\n",
    "            rows.append({\n",
    "                'Y': y,\n",
    "                'Z': z,\n",
    "                't': t\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def estimate_alpha_beta_sigma2(data):\n",
    "    X = data[['t', 'Z']]\n",
    "    y = data['Y']\n",
    "\n",
    "    weights = 1 / data['t']\n",
    "    model = LinearRegression().fit(X, y, sample_weight=weights)\n",
    "    alpha_hat = model.coef_[0]\n",
    "    beta_hat = model.coef_[1]\n",
    "\n",
    "    # Residual variance estimate of σ²\n",
    "\n",
    "    residuals = y - model.predict(X)\n",
    "    sigma_sq_hat = np.sum(weights * residuals**2) / len(y)\n",
    "\n",
    "    return alpha_hat, beta_hat, sigma_sq_hat\n",
    "\n",
    "root = 407\n",
    "n = 1000\n",
    "df = simulate_full_data(G, n, alpha=0.5, beta=1, sigma_sq=2500, alpha_0=50000, sigma_0_sq=5000)\n",
    "alpha_hat, beta_hat, sigma_sq_hat = estimate_alpha_beta_sigma2(df)\n",
    "\n",
    "print(\"Estimated alpha:\", alpha_hat)\n",
    "print(\"Estimated beta:\", beta_hat)\n",
    "print(\"Estimated sigma_sq:\", sigma_sq_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Estimated alpha: 0.5089949276253402\n",
      "Estimated beta: 1.0000112235371617\n",
      "Estimated sigma_sq: 2516.9592265481347\n",
      "Estimated alpha_0: 50128.78403878674\n",
      "Iteration: 4\n",
      "Estimated alpha: 0.5803006964062555\n",
      "Estimated beta: 1.0001149705577623\n",
      "Estimated sigma_sq: 2496.7100961142537\n",
      "Estimated alpha_0: 50104.94089359476\n",
      "Iteration: 8\n",
      "Estimated alpha: 0.6499466056609828\n",
      "Estimated beta: 1.0000751787922177\n",
      "Estimated sigma_sq: 2473.127344417362\n",
      "Estimated alpha_0: 50010.56971030272\n",
      "Iteration: 12\n",
      "Estimated alpha: 0.6630091757137117\n",
      "Estimated beta: 0.999970225726177\n",
      "Estimated sigma_sq: 2466.151616416253\n",
      "Estimated alpha_0: 49903.73638476002\n",
      "Iteration: 16\n",
      "Estimated alpha: 0.9023183025126851\n",
      "Estimated beta: 0.999899136491708\n",
      "Estimated sigma_sq: 2469.1180430915188\n",
      "Estimated alpha_0: 49737.78239748085\n",
      "Iteration: 20\n",
      "Estimated alpha: 0.910995508855802\n",
      "Estimated beta: 0.9998718594427254\n",
      "Estimated sigma_sq: 2493.798962706359\n",
      "Estimated alpha_0: 49536.32968339021\n",
      "Iteration: 24\n",
      "Estimated alpha: 0.9295327445654563\n",
      "Estimated beta: 0.9998380455362909\n",
      "Estimated sigma_sq: 2500.4269402042523\n",
      "Estimated alpha_0: 49426.28680828836\n",
      "Iteration: 28\n",
      "Estimated alpha: 0.8946303165092935\n",
      "Estimated beta: 0.9997775825456349\n",
      "Estimated sigma_sq: 2503.5529706322372\n",
      "Estimated alpha_0: 49235.22989259815\n",
      "Iteration: 32\n",
      "Estimated alpha: 0.9633746776442231\n",
      "Estimated beta: 0.9996831885630938\n",
      "Estimated sigma_sq: 2464.135452513711\n",
      "Estimated alpha_0: 48917.87726654549\n",
      "Iteration: 36\n",
      "Estimated alpha: 1.047620931849485\n",
      "Estimated beta: 0.9996216714550558\n",
      "Estimated sigma_sq: 2455.7621486828084\n",
      "Estimated alpha_0: 48723.7299906502\n",
      "Iteration: 40\n",
      "Estimated alpha: 1.009326126469384\n",
      "Estimated beta: 0.9995095082824215\n",
      "Estimated sigma_sq: 2504.8809210389086\n",
      "Estimated alpha_0: 48524.15111519021\n",
      "Iteration: 44\n",
      "Estimated alpha: 0.707992445675784\n",
      "Estimated beta: 0.9996957299857638\n",
      "Estimated sigma_sq: 2525.00724448036\n",
      "Estimated alpha_0: 48534.94094639274\n",
      "Iteration: 48\n",
      "Estimated alpha: 1.0953017463370656\n",
      "Estimated beta: 0.9998043604778603\n",
      "Estimated sigma_sq: 2524.589755289701\n",
      "Estimated alpha_0: 48636.458688351035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.1811298563617383, 0.9998123710231457, 2512.4431457027795, 48652.85007111289)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hard_assignment_EM(n, alpha_init, beta_init, sigma_sq_init, alpha_0_init, root=407):\n",
    "    alpha = alpha_init\n",
    "    beta = beta_init\n",
    "    sigma_sq = sigma_sq_init\n",
    "    alpha_0 = alpha_0_init\n",
    "\n",
    "    for i in range(n):\n",
    "        G = create_graph(tree, alpha, beta, sigma_sq)\n",
    "        df = simulate_full_data(G, 100, alpha, beta, sigma_sq, alpha_0, 5000)\n",
    "        single_dataset = df.iloc[:, 0]\n",
    "        leaves = [node for node in G.nodes if G.out_degree(node) == 0]\n",
    "        X_values = single_dataset[leaves]\n",
    "\n",
    "        J_hat_z, h_hat_z = inference_algorithm(G, alpha, beta, sigma_sq, alpha_0, X_values, root)\n",
    "        alpha_0_hat, _ = information_to_standard(J_hat_z, h_hat_z, root)\n",
    "        alpha_hat, beta_hat, sigma_sq_hat = estimate_alpha_beta_sigma2(df)\n",
    "\n",
    "        alpha = alpha_hat\n",
    "        beta = beta_hat\n",
    "        sigma_sq = sigma_sq_hat\n",
    "        alpha_0 = alpha_0_hat\n",
    "        if i % 4 == 0:\n",
    "            print(\"Iteration:\", i)\n",
    "            print(\"Estimated alpha:\", alpha)\n",
    "            print(\"Estimated beta:\", beta)\n",
    "            print(\"Estimated sigma_sq:\", sigma_sq)\n",
    "            print(\"Estimated alpha_0:\", alpha_0)  \n",
    "        \n",
    "    return alpha, beta, sigma_sq, alpha_0\n",
    "\n",
    "hard_assignment_EM(50, 0.5, 1, 2500, 50000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
